{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg2pEgMwfOxx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlyKK3MsbD7V",
        "outputId": "e3ff24da-9d43-4a79-de8c-09959ab33116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision pillow einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX5nDBrdbhsm",
        "outputId": "a812f053-d85d-4d35-dfda-693f63f1a834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmQKHDRRgKsO",
        "outputId": "d2b3821b-1a18-42e5-ff4c-4f782538a8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/XAI_project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/XAI_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzNqZITOAl9K"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhcsLxw1BNQr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU8Hcr87BETr"
      },
      "outputs": [],
      "source": [
        "from vit_model.baselines.ViT.ViT_explanation_generator_CPU import LRP\n",
        "from vit_model.VIT_LRP import vit_base_patch16_224_spectrogram as vit_LRP\n",
        "from PIL import Image\n",
        "from dataset.GTZAN import GTZAN\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from audio_preprocess.audio_preprocess import AudioPreprocessor\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from IPython.display import Audio, display\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from dataset.GTZAN_SNR import GTZAN_SNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXubNQCJBSmq"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXYnA1JGBUVq"
      },
      "outputs": [],
      "source": [
        "def add_noise_to_audio_get_spectrogram(audio, snr_db, audiopreprocessor):\n",
        "    rms_signal = math.sqrt(np.mean(audio ** 2))\n",
        "    rms_noise = rms_signal / (10 ** (snr_db / 20))\n",
        "    noise = np.random.normal(0, rms_noise, audio.shape[0])\n",
        "    spectrogram = audiopreprocessor.compute_log_spectrogram(audio + noise)\n",
        "    return spectrogram\n",
        "\n",
        "# create heatmap from mask on image\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return cam\n",
        "\n",
        "# Attribution visualization generation\n",
        "def generate_visualization(original_image, not_transformed_image, attribution_generator, class_index=None):\n",
        "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0), method=\"transformer_attribution\", index=class_index).detach() # (1, w*w)\n",
        "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14) # (1, 1, w, w)\n",
        "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, size=(1600, 224), mode='bilinear')\n",
        "    transformer_attribution = transformer_attribution.reshape(1600, 224).data.cpu().numpy()\n",
        "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
        "\n",
        "    image_transformer_attribution = not_transformed_image.permute(1, 2, 0).data.cpu().numpy()\n",
        "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * transformer_attribution), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(image_transformer_attribution)\n",
        "    cam = cam / np.max(cam)\n",
        "    vis =  np.uint8(255 * cam)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    return vis, transformer_attribution\n",
        "\n",
        "# Generating spectrogram tensor without attention parts\n",
        "def generate_removal_evaluation(original_image, not_transformed_image, attribution_generator, class_index=None, threshold=0.5):\n",
        "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0), method=\"transformer_attribution\", index=class_index).detach() # (1, w*w)\n",
        "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14) # (1, 1, w, w)\n",
        "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, size=(1600, 224), mode='bilinear')\n",
        "    transformer_attribution = transformer_attribution.reshape(1600, 224).data.cpu().numpy()\n",
        "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
        "\n",
        "    mask_TA = np.where(transformer_attribution < threshold, 1, 0)\n",
        "    original_tensor_filtered = not_transformed_image*mask_TA\n",
        "\n",
        "    return original_tensor_filtered\n",
        "\n",
        "# Print predicted classes\n",
        "def print_top_classes(predictions, class_map, **kwargs):\n",
        "    # Print Top-5 predictions\n",
        "    prob = torch.softmax(predictions, dim=1)\n",
        "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
        "    max_str_len = 0\n",
        "    class_names = []\n",
        "\n",
        "    for cls_idx in class_indices:\n",
        "        class_names.append(class_map[cls_idx])\n",
        "        if len(class_map[cls_idx]) > max_str_len:\n",
        "            max_str_len = len(class_map[cls_idx])\n",
        "\n",
        "    print('Top 5 classes:')\n",
        "    for cls_idx in class_indices:\n",
        "        output_string = '\\t{} : {}'.format(cls_idx, class_map[cls_idx])\n",
        "        output_string += ' ' * (max_str_len - len(class_map[cls_idx])) + '\\t\\t'\n",
        "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
        "        print(output_string)\n",
        "\n",
        "# Swapping label map to print predicted classes\n",
        "def swap_label_map(label_map):\n",
        "    label_map_int = {}\n",
        "\n",
        "    for key, value in label_map.items():\n",
        "        label_map_int[value] = key\n",
        "\n",
        "    return label_map_int\n",
        "\n",
        "#Sonification function\n",
        "def sonify(genre, number, transformer_attribution):\n",
        "  audio_f = f'data/genres_original/test/{GENRE}/{GENRE}.000{NUMBER}.wav'\n",
        "  audio_section = audiopreprocessor.load_audio(audio_f, select_section = False)\n",
        "  audio_section = audiopreprocessor.normalize_amplitude(audio_section)\n",
        "  stft_result = librosa.stft(audio_section, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "  output_audio = (transformer_attribution**2)*stft_result\n",
        "  # reconstructs audio form the intermediate result (stft) where phase information is preserved\n",
        "  reconstruction_audio = librosa.istft(output_audio, n_fft = N_FFT, hop_length=HOP_LENGTH)\n",
        "  output_audio_path = f'output_audios/output_audio_{GENRE}_{NUMBER}.wav'\n",
        "  sf.write(output_audio_path, reconstruction_audio, SAMPLE_RATE)\n",
        "  return audio_f, output_audio_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKlkEYHCH7z"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqUEGQaJCKxk"
      },
      "outputs": [],
      "source": [
        "GENRE = 'disco'\n",
        "NUMBER = '96'\n",
        "\n",
        "DPI = 100\n",
        "IM_DATA = plt.imread(f'data/images_1600_224/test/{GENRE}/{GENRE}000{NUMBER}.png')\n",
        "HEIGHT, WIDTH, _ = IM_DATA.shape\n",
        "\n",
        "FIGSIZE = (WIDTH / float(DPI))*2, HEIGHT / float(DPI)\n",
        "FIGSIZE_ONLY_MAP = (WIDTH / float(DPI)), HEIGHT / float(DPI)\n",
        "FIGSIZE_REMOVAL = (WIDTH / float(DPI))*3, HEIGHT / float(DPI)\n",
        "\n",
        "DATASET = GTZAN(mode='test', folder='images_1600_224')\n",
        "LABEL_MAP = swap_label_map(DATASET.label_map)\n",
        "\n",
        "transform_tensor = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transform_toPIL = transforms.Compose([\n",
        "    transforms.ToPILImage()\n",
        "])\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "MODEL = vit_LRP(pretrained=True)\n",
        "MODEL.eval()\n",
        "ATTRIBUTION_GENERATOR = LRP(MODEL)\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30\n",
        "HOP_LENGTH_FACTOR = 7.45\n",
        "N_FFT = 1599*2\n",
        "MONO = True\n",
        "HOP_LENGTH = int(SAMPLE_RATE / HOP_LENGTH_FACTOR)\n",
        "\n",
        "audiopreprocessor = AudioPreprocessor(SAMPLE_RATE, DURATION, HOP_LENGTH_FACTOR, N_FFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFjWLkcS--99"
      },
      "source": [
        "# Attribution Map generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GLZYDO3Cd1J",
        "outputId": "1a181271-860f-411a-fa45-59bdf758997b"
      },
      "outputs": [],
      "source": [
        "image_spec = Image.open(f'data/images_1600_224/test/{GENRE}/{GENRE}000{NUMBER}.png').convert('RGB')\n",
        "image_spec_tensor = transform_tensor(image_spec)\n",
        "image_spec_transf = DATASET.transform(image_spec)\n",
        "\n",
        "output = MODEL(image_spec_transf.unsqueeze(0))\n",
        "print_top_classes(output, LABEL_MAP)\n",
        "\n",
        "visualization, _ = generate_visualization(image_spec_transf, image_spec_tensor, ATTRIBUTION_GENERATOR)\n",
        "\n",
        "combined_image = plt.figure(figsize=FIGSIZE)\n",
        "\n",
        "ax1 = combined_image.add_subplot(1, 2, 1)\n",
        "ax2 = combined_image.add_subplot(1, 2, 2)\n",
        "\n",
        "ax1.imshow(image_spec)\n",
        "ax1.axis('off')\n",
        "ax2.imshow(visualization)\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.tight_layout(pad=0, h_pad=0, w_pad=0, rect=(0, 0, 0, 0))\n",
        "plt.savefig('combined_image.png')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Combined image saved as combined_image.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MmdPW7n8Cms"
      },
      "source": [
        "# Attribution Map Sonification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "38_gE6y98GLP",
        "outputId": "e76fe080-2d66-4c4b-bb70-3acebfad39e4"
      },
      "outputs": [],
      "source": [
        "GENRE = 'metal'\n",
        "NUMBER = '95'\n",
        "\n",
        "image_spec = Image.open(f'data/images_1600_224/test/{GENRE}/{GENRE}000{NUMBER}.png').convert('RGB')\n",
        "image_spec_tensor = transform_tensor(image_spec)\n",
        "image_spec_transf = DATASET.transform(image_spec)\n",
        "\n",
        "output = MODEL(image_spec_transf.unsqueeze(0))\n",
        "print_top_classes(output, LABEL_MAP)\n",
        "\n",
        "visualization, transformer_attribution = generate_visualization(image_spec_transf, image_spec_tensor, ATTRIBUTION_GENERATOR)\n",
        "\n",
        "map_image = plt.figure(figsize=FIGSIZE_ONLY_MAP)\n",
        "\n",
        "ax = map_image.add_subplot(1, 1, 1)\n",
        "ax.imshow(visualization)\n",
        "ax.axis('off')\n",
        "plt.tight_layout(pad=0, h_pad=0, w_pad=0, rect=(0, 0, 0, 0))\n",
        "\n",
        "audio_f, output_audio_path = sonify(GENRE, NUMBER, transformer_attribution)\n",
        "\n",
        "display(Audio(audio_f))\n",
        "display(Audio(output_audio_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyFo6LhDUjjd"
      },
      "source": [
        "# Evaluation feature removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k558MbYMUnUU",
        "outputId": "7af28963-c189-4e9c-efe8-1abdfddccb51"
      },
      "outputs": [],
      "source": [
        "GENRE = 'jazz'\n",
        "NUMBER = '95'\n",
        "\n",
        "image_spec = Image.open(f'data/images_1600_224/test/{GENRE}/{GENRE}000{NUMBER}.png').convert('RGB')\n",
        "image_spec_tensor = transform_tensor(image_spec)\n",
        "\n",
        "print(\"CLASSIFICATION BEFORE REMOVAL\")\n",
        "image_spec_transf = DATASET.transform(image_spec)\n",
        "output = MODEL(image_spec_transf.unsqueeze(0))\n",
        "print_top_classes(output, LABEL_MAP)\n",
        "\n",
        "visualization, _ = generate_visualization(image_spec_transf, image_spec_tensor, ATTRIBUTION_GENERATOR)\n",
        "image_filtered = generate_removal_evaluation(image_spec_transf, image_spec_tensor, ATTRIBUTION_GENERATOR, threshold=0.05)\n",
        "\n",
        "image_filtered_pil = transform_toPIL(image_filtered)\n",
        "\n",
        "print(\"CLASSIFICATION AFTER REMOVAL\")\n",
        "image_filtered_pil_transf = DATASET.transform(image_filtered_pil)\n",
        "output = MODEL(image_filtered_pil_transf.unsqueeze(0))\n",
        "print_top_classes(output, LABEL_MAP)\n",
        "\n",
        "visualization_filtered, transformer_attribution_filtered = generate_visualization(image_filtered_pil_transf, image_filtered, ATTRIBUTION_GENERATOR)\n",
        "\n",
        "image = plt.figure(figsize=FIGSIZE_REMOVAL)\n",
        "\n",
        "ax1 = image.add_subplot(1, 3, 1)\n",
        "ax2 = image.add_subplot(1, 3, 2) \n",
        "ax3 = image.add_subplot(1, 3, 3) \n",
        "ax1.imshow(visualization)\n",
        "ax1.axis('off')\n",
        "ax2.imshow(image_filtered_pil)\n",
        "ax2.axis('off')\n",
        "ax3.imshow(visualization_filtered)\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout(pad=0, h_pad=0, w_pad=0, rect=(0, 0, 0, 0))\n",
        "plt.savefig(f'spectrogram_{GENRE}_filtered.png')\n",
        "plt.show()\n",
        "\n",
        "audio_f, output_audio_path = sonify(GENRE, NUMBER, transformer_attribution_filtered)\n",
        "\n",
        "display(Audio(audio_f))\n",
        "display(Audio(output_audio_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlN0BRAI90ig"
      },
      "source": [
        "# Model Evaluation with Confusion Matrix generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxPfsU24dczz"
      },
      "outputs": [],
      "source": [
        "genre_corrects = {genre: 0 for genre in DATASET.label_map}\n",
        "genre_tot = {genre: 0 for genre in DATASET.label_map}\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "test_loader = DataLoader(DATASET, batch_size=32, shuffle=False)\n",
        "TEST_LABEL_MAP = DATASET.label_map\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = MODEL(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        for pred, label in zip(preds, labels):\n",
        "            print()\n",
        "            genre_corrects[LABEL_MAP[label.item()]] += int(pred == label)\n",
        "            genre_tot[LABEL_MAP[label.item()]] += 1\n",
        "\n",
        "genre_accuracy = {genre: genre_corrects[genre] / genre_tot[genre] for genre in TEST_LABEL_MAP}\n",
        "overall_accuracy = np.mean(list(genre_accuracy.values()))\n",
        "\n",
        "for genre in TEST_LABEL_MAP:\n",
        "    print(f\"{genre}: {genre_accuracy[genre]:.4f}\")\n",
        "\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "Np5jCVNcAWth",
        "outputId": "372309c9-568c-4540-b67d-6babc79b0215"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genre_tot.keys(), yticklabels=genre_tot.keys() ,cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('confusion_matrix.svg', format='svg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y18R0g2OWZDi"
      },
      "source": [
        "# Noise Injection 5-80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOCtA1cuOe3o",
        "outputId": "5e0d7a4c-4648-4290-852f-ab8df931fd34"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "snr_values = range(5, 101, 5)\n",
        "accuracies = []\n",
        "\n",
        "for snr in snr_values:\n",
        "    correct_total = 0\n",
        "    total_samples = 0\n",
        "    dataset = GTZAN(mode='', folder=f\"noisy_1600_224_snr{snr}\")\n",
        "    test_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = MODEL(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct_total += torch.sum(preds == labels).item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    overall_accuracy = correct_total / total_samples\n",
        "    accuracies.append(overall_accuracy)\n",
        "\n",
        "    print(f\"SNR {snr}: Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMKdKfnZhFTQ",
        "outputId": "1ae06d81-0da2-4b70-fdc7-83697b442581"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(snr_values, accuracies, marker='o')\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs SNR')\n",
        "plt.grid(True)\n",
        "plt.xticks(snr_values)\n",
        "plt.yticks(np.arange(0.3, 1, 0.1))\n",
        "plt.savefig('snr_vs_accuracy.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz_kx46SjHLw"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lg2pEgMwfOxx",
        "BzNqZITOAl9K",
        "OhcsLxw1BNQr",
        "dXubNQCJBSmq",
        "8FKlkEYHCH7z",
        "dFjWLkcS--99",
        "Q6TWIY7YQdgu",
        "DlN0BRAI90ig",
        "y18R0g2OWZDi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
